{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./project/Turtle_Soup\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "# r'.' not include \\n\n",
    "def remove_(df):\n",
    "    df = df.str.replace(r'\\n', ' ')\n",
    "    df = df.apply(lambda x: re.sub(r\" \\(.*?\\)\", \"\", x))\n",
    "    df = df.str.replace(r'[0-9].*?  ', \"\")\n",
    "    df = df.str.replace(r'[0-9].*?answer:  ', \"\")\n",
    "    df = df.str.replace(r'\\s+', ' ').str.replace(r'Variant: ', '')\n",
    "    df = df.str.replace(r'\\\\',\"\")\n",
    "    return df.str.lstrip().str.rstrip()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "data": {
      "text/plain": "    id                                             prompt  \\\n0    0  A man goes into a restaurant, orders abalone, ...   \n1    1  same problem statement but with albatross inst...   \n2    2   A man kills himself rather than order albatross.   \n3    3  A man lives on the twelfth floor of an apartme...   \n4    4  Emily regularly visits the twelfth floor of an...   \n5    5  A man sitting on a park bench reads a newspape...   \n6    6  A man lets go of a bowling ball. A short while...   \n7    7  A man is born in 1972 and dies in 1952 at the ...   \n8    8  A man is found dead in the arctic with a pack ...   \n9    9  A man pushes a car up to a hotel and tells the...   \n10  10  The car came out of the blue and the man came ...   \n11  11  A man lies dead in a room with fifty-three bic...   \n12  12          There are 53 bees instead of 53 bicycles.   \n13  13                        There are 51 instead of 53.   \n\n                                               answer  \n0   The man was in a ship that was wrecked on a de...  \n1   In this version, the man was in a lifeboat, wi...  \n2   The man already knew that he had eaten human f...  \n3   The man is a midget. He can't reach the upper ...  \n4   Emily is a child; she can only reach the tenth...  \n5   The man is a travel agent. He had sold someone...  \n6   A physics professor is demonstrating conservat...  \n7   He's born in room number The numbers can of co...  \n8   It's a wolf pack; they've killed and eaten the...  \n9                            It's a game of Monopoly.  \n10  The same; in this case the car token passes Go...  \n11  The \"bicycles\" are Bicycle playing cards; the ...  \n12                                          The same.  \n13  Someone saw the guy conceal a card, and proved...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>A man goes into a restaurant, orders abalone, ...</td>\n      <td>The man was in a ship that was wrecked on a de...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>same problem statement but with albatross inst...</td>\n      <td>In this version, the man was in a lifeboat, wi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>A man kills himself rather than order albatross.</td>\n      <td>The man already knew that he had eaten human f...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>A man lives on the twelfth floor of an apartme...</td>\n      <td>The man is a midget. He can't reach the upper ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Emily regularly visits the twelfth floor of an...</td>\n      <td>Emily is a child; she can only reach the tenth...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>A man sitting on a park bench reads a newspape...</td>\n      <td>The man is a travel agent. He had sold someone...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>A man lets go of a bowling ball. A short while...</td>\n      <td>A physics professor is demonstrating conservat...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>A man is born in 1972 and dies in 1952 at the ...</td>\n      <td>He's born in room number The numbers can of co...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>A man is found dead in the arctic with a pack ...</td>\n      <td>It's a wolf pack; they've killed and eaten the...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>A man pushes a car up to a hotel and tells the...</td>\n      <td>It's a game of Monopoly.</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>The car came out of the blue and the man came ...</td>\n      <td>The same; in this case the car token passes Go...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>A man lies dead in a room with fifty-three bic...</td>\n      <td>The \"bicycles\" are Bicycle playing cards; the ...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>There are 53 bees instead of 53 bicycles.</td>\n      <td>The same.</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>There are 51 instead of 53.</td>\n      <td>Someone saw the guy conceal a card, and proved...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/kith_dataset.csv\", index_col=0)\n",
    "data.prompt = remove_(data.prompt)\n",
    "data.answer = remove_(data.answer)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "data.to_csv(\"./data/kith_cleaned_dataset.csv\", index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "def create_finetuning_kith_data(df, filename):\n",
    "    pattern = r',|\\.|!|;'\n",
    "    fine_tuning_data = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt_start = \",\".join(re.split(pattern, row[\"answer\"])[:2])\n",
    "        prompt_end = row[\"prompt\"]\n",
    "        answer = row[\"answer\"]\n",
    "\n",
    "        data = {}\n",
    "        data['prompt'] = f\"[STORY START] {prompt_start}\\n\" +\\\n",
    "                         f\"[STORY END] {prompt_end}\\n\"\n",
    "        data['completion'] = f\"[STORY MIDDLE] {answer}\\n\" + \"[END]\"\n",
    "        fine_tuning_data.append(data)\n",
    "\n",
    "    with open(filename, 'w') as out:\n",
    "        for data in fine_tuning_data:\n",
    "            out.write(json.dumps(data))\n",
    "            out.write('\\n')\n",
    "\n",
    "jsonl_filename='fine_tune_kith_v4.jsonl'\n",
    "create_finetuning_kith_data(data, jsonl_filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prompt\": \"[STORY START] The man was in a ship that was wrecked on a desert island, When there was no food left\\n[STORY END] A man goes into a restaurant, orders abalone, eats one bite, and kills himself.\\n\", \"completion\": \"[STORY INFERENCE] The man was in a ship that was wrecked on a desert island. When there was no food left, another passenger brought what he said was abalone but was really part of the man's wife. The man suspects something fishy, so when they finally return to civilization, he orders abalone, realizes that what he ate before was his wife, and kills himself.\\n[END]\"}\r\n",
      "{\"prompt\": \"[STORY START] In this version, the man was in a lifeboat\\n[STORY END] same problem statement but with albatross instead of abalone.\\n\", \"completion\": \"[STORY INFERENCE] In this version, the man was in a lifeboat, with his wife, who died. He hallucinated an albatross landing in the boat which he caught and killed and ate; he thought that his wife had been washed overboard. When he actually eats albatross, he discovers that he had actually eaten his wife.\\n[END]\"}\r\n",
      "{\"prompt\": \"[STORY START] The man already knew that he had eaten human flesh under the name \\\"albatross,\\\" He asks the waiter in the restaurant what kind of soup is available\\n[STORY END] A man kills himself rather than order albatross.\\n\", \"completion\": \"[STORY INFERENCE] The man already knew that he had eaten human flesh under the name \\\"albatross.\\\" He asks the waiter in the restaurant what kind of soup is available, and the waiter responds, \\\"Albatross soup.\\\" Thinking that \\\"albatross soup\\\" means \\\"human soup,\\\" and sickened by the thought of such a society, he kills himself. I'm afraid this version doesn't make a whole lot of sense.\\n[END]\"}\r\n",
      "{\"prompt\": \"[STORY START] The man is a midget, He can't reach the upper elevator buttons\\n[STORY END] A man lives on the twelfth floor of an apartment building. Every morning he takes the elevator down to the lobby and leaves the building. In the evening, he gets into the elevator, and, if there is someone else in the elevator -- or if it was raining that day -- he goes back to his floor directly. However, if there is nobody else in the elevator and it hasn't rained, he goes to the tenth floor and walks up two flights of stairs to his room.\\n\", \"completion\": \"[STORY INFERENCE] The man is a midget. He can't reach the upper elevator buttons, but he can ask people to push them for him. He can also push them with his umbrella. I've usually heard this stated with more details: \\\"Every morning he wakes up, gets dressed, eats, goes to the elevator...\\\" In the other direction, for a shorter problem statement, leave out the \\\"someone else in the elevator\\\" and \\\"if it was raining\\\" parts, and just say on his return to the building he always goes to the tenth floor. Ron Carter suggests a nice red herring: the man lives on the 13th floor of the building.\\n[END]\"}\r\n",
      "{\"prompt\": \"[STORY START] Emily is a child, she can only reach the tenth-floor button\\n[STORY END] Emily regularly visits the twelfth floor of an apartment building by going to the tenth floor and walking up two flights of stairs. Last year she only took the elevator to the ninth floor.\\n\", \"completion\": \"[STORY INFERENCE] Emily is a child; she can only reach the tenth-floor button, and last year she could only reach the ninth-floor button.\\n[END]\"}\r\n",
      "{\"prompt\": \"[STORY START] The man is a travel agent, He had sold someone two tickets for an ocean voyage\\n[STORY END] A man sitting on a park bench reads a newspaper article headlined \\\"Death at Sea\\\" and knows a murder has been committed.\\n\", \"completion\": \"[STORY INFERENCE] The man is a travel agent. He had sold someone two tickets for an ocean voyage, one round-trip and one one-way. The last name of the man who bought the tickets is the same as the last name of the woman who \\\"fell\\\" overboard and drowned on the same voyage, which is the subject of the article he's reading. This may have derived from a story done by Alfred Hitchcock, if the following Hitchcock quotation is accurate: \\\"If you take your wife on a sea voyage, buy her a round-trip ticket no matter what your plans may be.\\\" According to How Come?, it's loosely based on the real-life case of a killer named Henry Landru.\\n[END]\"}\r\n",
      "{\"prompt\": \"[STORY START] A physics professor is demonstrating conservation of energy by suspending a bowling ball from a piece of rope, He pulls the ball back until it's right in front of his nose\\n[STORY END] A man lets go of a bowling ball. A short while later, he is rushed to the hospital.\\n\", \"completion\": \"[STORY INFERENCE] A physics professor is demonstrating conservation of energy by suspending a bowling ball from a piece of rope. He pulls the ball back until it's right in front of his nose, then lets go. It is supposed to swing away from him, then back at him, stopping just in front of his nose. Unfortunately, he gave the ball a slight push, resulting in the ball crashing into his nose upon its return.\\n[END]\"}\r\n",
      "{\"prompt\": \"[STORY START] He's born in room number The numbers can of course vary, it was originally set up with those numbers reversed\\n[STORY END] A man is born in 1972 and dies in 1952 at the age of 25.\\n\", \"completion\": \"[STORY INFERENCE] He's born in room number The numbers can of course vary; it was originally set up with those numbers reversed, but I like it better this way.\\n[END]\"}\r\n",
      "{\"prompt\": \"[STORY START] It's a wolf pack, they've killed and eaten the man\\n[STORY END] A man is found dead in the arctic with a pack on his back.\\n\", \"completion\": \"[STORY INFERENCE] It's a wolf pack; they've killed and eaten the man.\\n[END]\"}\r\n",
      "{\"prompt\": \"[STORY START] It's a game of Monopoly,\\n[STORY END] A man pushes a car up to a hotel and tells the owner he's bankrupt.\\n\", \"completion\": \"[STORY INFERENCE] It's a game of Monopoly.\\n[END]\"}\r\n",
      "  14 1229 fine_tune_kith_v4.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!head '{jsonl_filename}'\n",
    "!wc -lw '{jsonl_filename}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finetune"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "print('Enter OpenAI API key:')\n",
    "openai.api_key = input()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=openai.api_key"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging requires wandb to be installed. Run `pip install wandb`.\r\n",
      "Upload progress: 100%|████████████████████| 7.22k/7.22k [00:00<00:00, 5.56Mit/s]\r\n",
      "Uploaded file from fine_tune_kith_v4.jsonl: file-BV2hUvn0gfP92uCr5AOxD1dd\r\n",
      "Created fine-tune: ft-AzlRmKQVs2WqXiSFWNr7sxkW\r\n",
      "Streaming events until fine-tuning is complete...\r\n",
      "\r\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\r\n",
      "[2022-04-05 19:59:15] Created fine-tune: ft-AzlRmKQVs2WqXiSFWNr7sxkW\r\n",
      "[2022-04-05 19:59:23] Fine-tune costs $0.20\r\n",
      "[2022-04-05 19:59:23] Fine-tune enqueued. Queue number: 2\r\n",
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.create -t '{jsonl_filename}' -m davinci\n",
    "#!openai api fine_tunes.create -t '{jsonl_filename}' -m davinci"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging requires wandb to be installed. Run `pip install wandb`.\r\n",
      "[2022-04-05 19:59:15] Created fine-tune: ft-AzlRmKQVs2WqXiSFWNr7sxkW\r\n",
      "[2022-04-05 19:59:23] Fine-tune costs $0.20\r\n",
      "[2022-04-05 19:59:23] Fine-tune enqueued. Queue number: 2\r\n",
      "[2022-04-05 20:06:41] Fine-tune is in the queue. Queue number: 0\r\n",
      "[2022-04-05 20:22:53] Fine-tune started\r\n",
      "[2022-04-05 20:37:52] Completed epoch 1/4\r\n",
      "[2022-04-05 20:37:57] Completed epoch 2/4\r\n",
      "[2022-04-05 20:38:03] Completed epoch 3/4\r\n",
      "[2022-04-05 20:38:09] Completed epoch 4/4\r\n",
      "[2022-04-05 20:38:40] Uploaded model: davinci:ft-personal-2022-04-06-00-38-38\r\n",
      "[2022-04-05 20:38:44] Uploaded result file: file-ysRetMLaRrGc0GqUiZhTrkvZ\r\n",
      "[2022-04-05 20:38:44] Fine-tune succeeded\r\n",
      "\r\n",
      "Job complete! Status: succeeded 🎉\r\n",
      "Try out your fine-tuned model:\r\n",
      "\r\n",
      "openai api completions.create -m davinci:ft-personal-2022-04-06-00-38-38 -p <YOUR_PROMPT>\r\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.follow -i ft-AzlRmKQVs2WqXiSFWNr7sxkW"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [],
   "source": [
    "def generate_finetuned_kith(prompt_start, prompt_end, max_tokens=256):\n",
    "  response = openai.Completion.create(\n",
    "    model='davinci:ft-personal-2022-04-06-00-38-38',\n",
    "    prompt=f\"[STORY START] {prompt_start}\\n\"+ f\"[STORY END] {prompt_end}\\n\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=max_tokens,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=['[END]']\n",
    "  )\n",
    "\n",
    "  # Make an API call to GPT3\n",
    "  turn = response['choices'][0]['text']\n",
    "  return turn\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "That model is still being loaded. Please try again shortly.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_327131/4251358496.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m print(generate_finetuned_kith(\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mprompt_start\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"The man was in a ship that was wrecked on a desert island, When there was no food left\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     prompt_end=\"A man goes into a restaurant, orders abalone, eats one bite, and kills himself\"))\n",
      "\u001B[0;32m/tmp/ipykernel_327131/3687472016.py\u001B[0m in \u001B[0;36mgenerate_finetuned_kith\u001B[0;34m(prompt_start, prompt_end, max_tokens)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mgenerate_finetuned_kith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprompt_start\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprompt_end\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_tokens\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m256\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m   response = openai.Completion.create(\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0mmodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'davinci:ft-personal-2022-04-06-00-38-38'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mprompt\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34mf\"[STORY START] {prompt_start}\\n\"\u001B[0m\u001B[0;34m+\u001B[0m \u001B[0;34mf\"[STORY END] {prompt_end}\\n\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mtemperature\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.7\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/openai/api_resources/completion.py\u001B[0m in \u001B[0;36mcreate\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     29\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 31\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     32\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mTryAgain\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mstart\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\u001B[0m in \u001B[0;36mcreate\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    103\u001B[0m         )\n\u001B[1;32m    104\u001B[0m         \u001B[0murl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclass_url\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mapi_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mapi_version\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 105\u001B[0;31m         response, _, api_key = requestor.request(\n\u001B[0m\u001B[1;32m    106\u001B[0m             \u001B[0;34m\"post\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m             \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(self, method, url, params, headers, files, stream, request_id)\u001B[0m\n\u001B[1;32m    118\u001B[0m             \u001B[0mrequest_id\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrequest_id\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m         )\n\u001B[0;32m--> 120\u001B[0;31m         \u001B[0mresp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgot_stream\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_interpret_response\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    121\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mresp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgot_stream\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapi_key\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001B[0m in \u001B[0;36m_interpret_response\u001B[0;34m(self, result, stream)\u001B[0m\n\u001B[1;32m    325\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    326\u001B[0m             return (\n\u001B[0;32m--> 327\u001B[0;31m                 self._interpret_response_line(\n\u001B[0m\u001B[1;32m    328\u001B[0m                     \u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus_code\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mheaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    329\u001B[0m                 ),\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001B[0m in \u001B[0;36m_interpret_response_line\u001B[0;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[1;32m    354\u001B[0m         \u001B[0mstream_error\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstream\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"error\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mresp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    355\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mstream_error\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;36m200\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mrcode\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m300\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 356\u001B[0;31m             raise self.handle_error_response(\n\u001B[0m\u001B[1;32m    357\u001B[0m                 \u001B[0mrbody\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrcode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrheaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream_error\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstream_error\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    358\u001B[0m             )\n",
      "\u001B[0;31mRateLimitError\u001B[0m: That model is still being loaded. Please try again shortly."
     ]
    }
   ],
   "source": [
    "print(generate_finetuned_kith(\n",
    "    prompt_start=\"The man was in a ship that was wrecked on a desert island, When there was no food left\",\n",
    "    prompt_end=\"A man goes into a restaurant, orders abalone, eats one bite, and kills himself\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}